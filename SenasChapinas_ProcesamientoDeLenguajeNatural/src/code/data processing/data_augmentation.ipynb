{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Señas Chapinas: Traductor de LENSEGUA**\n",
    "#### *Módulo de Procesamiento de Lenguaje Natural*\n",
    "\n",
    "Stefano Alberto Aragoni Maldonado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "#### **Data Augmentation**\n",
    "\n",
    "En la fase inicial del proyecto, se recopilaron frases en español que utilizaban la gramática de LENSEGUA. Estas frases fueron almacenadas en un archivo CSV junto con sus contrapartes gramaticalmente correctas en español.\n",
    "\n",
    "Estas frases se utilizarán para fine-tunear un LLM pre-entrenado, con el objetivo de que este modelo pueda asimilar la gramática de LENSEGUA. A través de esto, se espera que pueda interpretar oraciones que utilicen dicha gramática y las escriba correctamente en español.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para este proceso se necesitan muchas oraciones, por lo cual se plantea aplicar data augmentation. Esto se logrará modificando las oraciones según diferentes variables como el tiempo verbal, los lugares, los sujetos, y otros elementos gramaticales. De esta manera, se generará una mayor cantidad de datos de entrenamiento, lo que permitirá al modelo preentrenado mejorar su capacidad de comprensión y generación de texto utilizando la gramática de LENSEGUA.\n",
    "\n",
    "El proceso de data augmentation incluirá varias técnicas específicas. Una de ellas será la sustitución de palabras por sinónimos, lo que permitirá mantener el significado de las frases mientras se introducen variaciones léxicas. Otra técnica consistirá en cambiar el tiempo verbal de las oraciones, transformando frases en presente a pasado o futuro, y viceversa. Adicionalmente, se podrán alterar los lugares y sujetos de las oraciones para generar diferentes contextos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "#### *Importar librerías*\n",
    "Como primer paso, se importan las librerías necesarias para el desarrollo del problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk import ngrams, CFG\n",
    "from nltk.corpus import wordnet\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from regex import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "#### *Cargar el dataset*\n",
    "\n",
    "Luego, se carga el dataset que contiene las frases en español y su contraparte en LENSEGUA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LENSEGUA</th>\n",
       "      <th>ESPAÑOL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abuela 70 años tener</td>\n",
       "      <td>mi abuela tiene 70 años.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abuelo 70 años tener</td>\n",
       "      <td>mi abuelo tiene 70 años.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abuelo enfermo mucho</td>\n",
       "      <td>mi abuelo está muy enfermo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aeropuerto dónde pregunta</td>\n",
       "      <td>¿dónde está el aeropuerto?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeropuerto yo ir cuál pregunta</td>\n",
       "      <td>¿a cuál aeropuerto tengo que ir?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ahora tu libro leer cuál pregunta</td>\n",
       "      <td>¿cuál libro estás leyendo ahora?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>alegre señas aprender y hacer</td>\n",
       "      <td>es alegre aprender y hacer señas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>amiga desaparecer tu información compartir</td>\n",
       "      <td>mi amiga desapareció. comparte la información.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>anillo precio cuánto pregunta</td>\n",
       "      <td>¿cuánto cuesta el anillo?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>año pasado ella carro aprender</td>\n",
       "      <td>ella aprendió a manejar el año pasado.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>antes banco tu ir dinero quitar</td>\n",
       "      <td>fuiste al banco para retirar dinero.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>antes biblioteca tu ir para estudiar</td>\n",
       "      <td>fuiste a la biblioteca para estudiar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>antes cine yo querer ir pero cerrado</td>\n",
       "      <td>quería ir al cine pero estaba cerrado.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>antes escuela tu profesora ver pregunta</td>\n",
       "      <td>¿viste a la profesora en la escuela?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>antes farmacia tu ir para pastillas comprar</td>\n",
       "      <td>fuiste a la farmacia para comprar pastillas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>antes grande ahora pequeño</td>\n",
       "      <td>antes era grande ahora es pequeño.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>antes hoy piscina tu ir pregunta</td>\n",
       "      <td>¿fuiste a la piscina hoy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>antes hoy tu almorzar qué pregunta</td>\n",
       "      <td>¿qué almorzaste hoy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>antes museo tu ir pregunta</td>\n",
       "      <td>¿fuiste al museo?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>antes playa tu ir pregunta</td>\n",
       "      <td>¿fuiste a la playa?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       LENSEGUA  \\\n",
       "0                          abuela 70 años tener   \n",
       "1                          abuelo 70 años tener   \n",
       "2                          abuelo enfermo mucho   \n",
       "3                     aeropuerto dónde pregunta   \n",
       "4                aeropuerto yo ir cuál pregunta   \n",
       "5             ahora tu libro leer cuál pregunta   \n",
       "6                 alegre señas aprender y hacer   \n",
       "7    amiga desaparecer tu información compartir   \n",
       "8                 anillo precio cuánto pregunta   \n",
       "9                año pasado ella carro aprender   \n",
       "10              antes banco tu ir dinero quitar   \n",
       "11         antes biblioteca tu ir para estudiar   \n",
       "12         antes cine yo querer ir pero cerrado   \n",
       "13      antes escuela tu profesora ver pregunta   \n",
       "14  antes farmacia tu ir para pastillas comprar   \n",
       "15                   antes grande ahora pequeño   \n",
       "16             antes hoy piscina tu ir pregunta   \n",
       "17           antes hoy tu almorzar qué pregunta   \n",
       "18                   antes museo tu ir pregunta   \n",
       "19                   antes playa tu ir pregunta   \n",
       "\n",
       "                                           ESPAÑOL  \n",
       "0                         mi abuela tiene 70 años.  \n",
       "1                         mi abuelo tiene 70 años.  \n",
       "2                      mi abuelo está muy enfermo.  \n",
       "3                       ¿dónde está el aeropuerto?  \n",
       "4                 ¿a cuál aeropuerto tengo que ir?  \n",
       "5                 ¿cuál libro estás leyendo ahora?  \n",
       "6                es alegre aprender y hacer señas.  \n",
       "7   mi amiga desapareció. comparte la información.  \n",
       "8                        ¿cuánto cuesta el anillo?  \n",
       "9           ella aprendió a manejar el año pasado.  \n",
       "10            fuiste al banco para retirar dinero.  \n",
       "11           fuiste a la biblioteca para estudiar.  \n",
       "12          quería ir al cine pero estaba cerrado.  \n",
       "13            ¿viste a la profesora en la escuela?  \n",
       "14    fuiste a la farmacia para comprar pastillas.  \n",
       "15              antes era grande ahora es pequeño.  \n",
       "16                       ¿fuiste a la piscina hoy?  \n",
       "17                            ¿qué almorzaste hoy?  \n",
       "18                               ¿fuiste al museo?  \n",
       "19                             ¿fuiste a la playa?  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../../dataset/raw/dataset.csv\")                               # Load the dataset\n",
    "\n",
    "data[\"LENSEGUA\"] = data[\"LENSEGUA\"].apply(lambda x: x.lower())  # Convert to lowercase\n",
    "data[\"ESPAÑOL\"] = data[\"ESPAÑOL\"].apply(lambda x: x.lower())    # Convert to lowercase\n",
    "\n",
    "data[\"LENSEGUA\"] = data[\"LENSEGUA\"].apply(lambda x: re.sub(r'\"', '', x))    # Remove double quotes\n",
    "data[\"ESPAÑOL\"] = data[\"ESPAÑOL\"].apply(lambda x: re.sub(r'\"', '', x))      # Remove double quotes\n",
    "\n",
    "data.head(20)                                                   # Display the first 20 rows of the dataset         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente, se separa el dataset 80% para entrenamiento y 20% para validación. Solo al dataset de entrenamiento se le aplicarán las técnicas de data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)               # Shuffle data\n",
    "\n",
    "train_size = int(0.8 * len(data))                               # Calculate size of training set\n",
    "\n",
    "train_data = data[:train_size]                                  # Split data into training and validation sets\n",
    "validation_data = data[train_size:]                             # Training set is 80% of data, validation set is 20% of data\n",
    "\n",
    "\n",
    "# ------------------------ \n",
    "data = train_data                                               # Set data to training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "#### *Equivalencias de palabras*\n",
    "\n",
    "Posteriormente, se definen las equivalencias de palabras que se utilizarán para realizar la sustitución de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Places ------\n",
    "\n",
    "masc_home_places = [\"cuarto\", \"baño\", \"comedor\", \"garage\", \"sótano\", \"balcón\", \"pasillo\", \"estudio\", \"vestíbulo\", \"closet\"]\n",
    "fem_home_places = [\"cocina\", \"sala\", \"habitación\", \"terraza\", \"bodega\", \"lavandería\"]\n",
    "\n",
    "masc_outdoor_places = [\"jardín\", \"patio\", \"parque\", \"río\", \"lago\", \"bosque\", \"desierto\", \"volcán\", \"cerro\", \"cementerio\"]\n",
    "fem_outdoor_places = [\"playa\", \"montaña\", \"cascada\", \"cueva\", \"selva\", \"pradera\", \"isla\", \"laguna\", \"granja\", \"colina\", \"finca\", \"reserva\"]\n",
    "\n",
    "masc_work_places = [\"estudio\", \"consultorio\", \"cubículo\", \"salón\", \"laboratorio\", \"colegio\", \"taller\", \"instituto\"]\n",
    "fem_work_places = [\"oficina\", \"escuela\", \"universidad\", \"biblioteca\", \"fábrica\"]\n",
    "\n",
    "masc_sleep_places = [\"hotel\", \"motel\", \"hostal\", \"albergue\", \"apartamento\", \"bungaló\", \"chalé\"]\n",
    "fem_sleep_places = [\"casa\", \"residencia\", \"mansión\", \"cabaña\", \"choza\", \"villa\"]\n",
    "\n",
    "masc_tourist_places = [\"museo\", \"zoológico\", \"monumento\", \"templo\", \"cine\", \"estadio\", \"teatro\", \"acuario\", \"mirador\", \"puente\"]\n",
    "fem_tourist_places = [\"plaza\", \"catedral\", \"iglesia\", \"capilla\", \"pirámide\", \"ruina\", \"fuente\", \"feria\"]\n",
    "\n",
    "masc_eating_places = [\"restaurante\", \"bistro\", \"bar\", \"comedor\"]\n",
    "fem_eating_places = [\"cafetería\", \"taquería\", \"cevichería\", \"pizzería\", \"pastelería\", \"sandwichería\", \"heladería\", \"cantina\"]\n",
    "\n",
    "masc_shopping_food_places = [\"almacén\", \"mercado\", \"supermercado\"]\n",
    "fem_shopping_food_places = [\"tienda\", \"panadería\", \"carnicería\", \"pescadería\", \"frutería\", \"verdulería\"]\n",
    "\n",
    "masc_health_places = [\"hospital\", \"consultorio\", \"laboratorio\", \"sanatorio\", \"quirófano\"]\n",
    "fem_health_places = [\"clínica\", \"farmacia\", \"enfermería\", \"ambulancia\"]\n",
    "\n",
    "masc_shopping_places = [\"bazaar\", \"outlet\"]\n",
    "fem_shopping_places = [\"boutique\", \"joyería\", \"librería\", \"florería\", \"ferretería\", \"papelería\", \"perfumería\"]\n",
    "\n",
    "masc_transport_places = [\"aeropuerto\", \"puerto\", \"parqueo\", \"estacionamiento\", \"muelle\", \"hangar\", \"heliopuerto\"]\n",
    "fem_transport_places = [\"estación\", \"parada\", \"terminal\", \"autopista\", \"carretera\", \"calle\", \"avenida\"]\n",
    "\n",
    "# ------ People ------\n",
    "masc_family_older = [\"abuelo\", \"tío\", \"papá\", \"padrino\", \"padre\"]\n",
    "fem_family_older = [\"abuela\", \"tía\", \"mamá\", \"madrina\", \"madre\"]\n",
    "\n",
    "masc_family_younger = [\"hermano\", \"primo\", \"sobrino\", \"hijo\"]\n",
    "fem_family_younger = [\"hermana\", \"prima\", \"sobrina\", \"hija\"]\n",
    "\n",
    "masc_teachers = [\"maestro\", \"profesor\", \"director\", \"rector\", \"licenciado\", \"ingeniero\", \"arquitecto\", \"abogado\"]\n",
    "fem_teachers = [\"maestra\", \"profesora\", \"directora\", \"rectora\", \"licenciada\", \"ingeniera\", \"arquitecta\", \"abogada\"]\n",
    "\n",
    "masc_friends = [\"amigo\", \"compañero\", \"colega\", \"vecino\", \"conocido\", \"amante\", \"novio\", \"esposo\", \"amigo\"]\n",
    "fem_friends = [\"amiga\", \"compañera\", \"colega\", \"vecina\", \"conocida\", \"amante\", \"novia\", \"esposa\", \"amiga\"]\n",
    "\n",
    "masc_medical_emergency = [\"doctor\", \"médico\", \"enfermero\", \"cirujano\", \"odontólogo\", \"psicólogo\", \"cardiólogo\", \"neurólogo\", \"pediatra\", \"internista\"]\n",
    "fem_medical_emergency = [\"doctora\", \"médica\", \"enfermera\", \"cirujana\", \"odontóloga\", \"psicóloga\", \"cardióloga\", \"neuróloga\", \"pediatra\", \"internista\"]\n",
    "\n",
    "masc_first_responders = [\"policía\", \"bombero\", \"paramédico\", \"guardia\", \"rescatista\"]\n",
    "fem_first_responders = [\"bombera\", \"paramédica\", \"guardia\", \"rescatista\"]\n",
    "\n",
    "masc_first_responders_plural = [\"policías\", \"bomberos\", \"paramédicos\", \"guardias\", \"rescatistas\"]\n",
    "fem_first_responders_plural = [\"bomberas\", \"paramédicas\", \"guardias\", \"rescatistas\"]\n",
    "\n",
    "masc_professions = [\"taxista\", \"mesero\", \"camarero\", \"electricista\", \"plomero\", \"pintor\", \"jardinero\", \"chef\", \"piloto\", \"actor\", \"cantante\", \"escritor\", \"bailarín\", \"atleta\", \"científico\"]\n",
    "fem_professions = [\"camarera\", \"electricista\", \"fontanera\", \"pintora\", \"jardinera\", \"pilota\", \"actriz\", \"cantante\", \"escritora\", \"bailarina\", \"atleta\", \"científica\"]\n",
    "\n",
    "# ------ Objects ------\n",
    "\n",
    "masc_id_objects = [\"DPI\", \"pasaporte\", \"carné\"]\n",
    "fem_id_objects = [\"licencia\", \"tarjeta\", \"credencial\", \"cédula\"]\n",
    "\n",
    "masc_readable_objects = [\"libro\", \"periódico\", \"cómic\", \"diccionario\", \"cuaderno\"]\n",
    "fem_readable_objects = [\"revista\", \"novela\", \"biografía\", \"autobiografía\"]\n",
    "\n",
    "masc_readable_objects_plural = [\"libros\", \"periódicos\", \"cómics\", \"diccionarios\", \"cuadernos\"]\n",
    "fem_readable_objects_plural = [\"revistas\", \"novelas\", \"biografías\", \"autobiografías\"]\n",
    "\n",
    "masc_personal_objects = [\"teléfono\", \"reloj\", \"mapa\", \"paraguas\", \"cargador\", \"llavero\"]\n",
    "fem_personal_objects = [\"laptop\", \"computadora\", \"tablet\", \"cartera\", \"llave\", \"mochila\", \"cámara\", \"billetera\", \"maleta\", \"bolsa\", \"mascarilla\"]\n",
    "\n",
    "masc_dinero_objects = [\"dinero\", \"billetes\", \"monedas\"]\n",
    "\n",
    "masc_toys = [\"balón\", \"rompecabezas\", \"carrito\", \"lego\", \"peluche\", \"robot\", \"dinosaurio\"]\n",
    "fem_toys = [\"muñeca\", \"cocinita\", \"pelota\", \"barbie\"]\n",
    "\n",
    "\n",
    "masc_objects_transport = [\"avión\", \"barco\", \"tren\", \"helicóptero\", \"camión\", \"bus\", \"metro\", \"tractor\", \"scooter\"]\n",
    "fem_objects_transport = [\"bicicleta\", \"motocicleta\", \"patineta\", \"lancha\", \"canoa\", \"balsa\"]\n",
    "\n",
    "masc_homework_work_tools = [\"lápiz\", \"borrador\", \"cuaderno\", \"papel\", \"pegamento\", \"sacapuntas\", \"marcador\", \"corrector\", \"clip\", \"pincel\"]\n",
    "fem_homework_work_tools = [\"pluma\", \"libreta\", \"regla\", \"calculadora\", \"impresora\", \"engrapadora\", \"perforadora\", \"agenda\"]\n",
    "\n",
    "masc_work = [\"informe\", \"reporte\", \"ensayo\", \"proyecto\", \"examen\", \"documento\", \"resumen\", \"artículo\"]\n",
    "fem_work = [\"presentación\", \"exposición\", \"tarea\", \"investigación\", \"tesis\", \"disertación\"]\n",
    "\n",
    "masc_house_pets = [\"perro\", \"gato\", \"pez\", \"pájaro\", \"conejo\", \"hámster\", \"cuyo\", \"hurón\", \"erizo\", \"gecko\"]\n",
    "fem_house_pets = [\"iguana\", \"serpiente\", \"tortuga\", \"tarántula\", \"araña\", \"rana\", \"salamandra\", \"cacatúa\", \"guacamaya\"]\n",
    "\n",
    "masc_wild_animals = [\"león\", \"tigre\", \"oso\", \"elefante\", \"mono\", \"lobo\", \"zorro\", \"hipopótamo\", \"rinoceronte\", \"antílope\"]\n",
    "fem_wild_animals = [\"jirafa\", \"cebra\", \"pantera\", \"cabra\", \"oveja\", \"vaca\", \"gacela\", \"leona\", \"cebra\"]\n",
    "\n",
    "masc_insects = [\"mosquito\", \"avispón\", \"escarabajo\", \"grillo\", \"saltamontes\", \"escorpión\", \"ciempiés\"]\n",
    "fem_insects = [\"mosca\", \"abeja\", \"avispa\", \"hormiga\", \"cucaracha\", \"mariposa\", \"libélula\", \"polilla\", \"oruga\"]\n",
    "\n",
    "# ------ Events ------\n",
    "masc_events = [\"concierto\", \"festival\", \"carnaval\", \"bautizo\", \"desfile\", \"partido\"]\n",
    "fem_events = [\"fiesta\", \"celebración\"]\n",
    "\n",
    "\n",
    "# ------ Food ------\n",
    "masc_fruits_vegetables_singular = [\"tomate\", \"aguacate\", \"plátano\", \"mango\", \"coco\", \"limón\", \"durazno\", \"melón\", \"kiwi\", \"elote\", \"maíz\"]\n",
    "fem_fruits_vegetables_singular = [\"manzana\", \"pera\", \"uva\", \"sandía\", \"papaya\", \"piña\", \"fresa\", \"zanahoria\", \"papa\", \"cebolla\", \"lechuga\"]\n",
    "\n",
    "masc_fruits_vegetables_plural = [\"tomates\", \"aguacates\", \"plátanos\", \"mangos\", \"cocos\", \"limones\", \"duraznos\", \"melones\", \"kiwis\", \"elotes\", \"maíces\"]\n",
    "fem_fruits_vegetables_plural = [\"manzanas\", \"peras\", \"uvas\", \"sandías\", \"papayas\", \"piñas\", \"fresas\", \"zanahorias\", \"papas\", \"cebollas\", \"lechugas\"]\n",
    "\n",
    "masc_meat = [\"pollo\", \"carne\", \"cerdo\", \"pescado\", \"jamón\", \"chorizo\", \"salami\", \"pepperoni\"]\n",
    "fem_meat = [\"pechuga\", \"costilla\", \"chuleta\", \"salchicha\", \"longaniza\"]\n",
    "\n",
    "masc_meal = [\"estofado\", \"caldo\", \"hotdog\", \"sushi\", \"taco\", \"muffin\", \"pan\"]\n",
    "fem_meal = [\"ensalada\", \"sopa\", \"pizza\", \"hamburguesa\", \"torta\", \"tostada\", \"tortilla\"]\n",
    "\n",
    "meals_masc = [\"desayuno\", \"almuerzo\"]\n",
    "meals_fem = [\"cena\", \"merienda\"]\n",
    "\n",
    "masc_drinks = [\"refresco\", \"jugo\", \"té\", \"café\", \"agua\", \"vino\", \"whisky\", \"ron\", \"vodka\", \"tequila\", \"mezcal\", \"pulque\", \"licor\", \"brandy\"]\n",
    "fem_drinks = [\"leche\", \"cerveza\", \"champagne\", \"sidra\", \"limonada\", \"horchata\", \"michelada\", \"margarita\"]\n",
    "\n",
    "# ------ Feelings ------\n",
    "neutral_feelings = [\"feliz\", \"triste\"]\n",
    "\n",
    "masc_feelings = [\"enojado\", \"cansado\", \"sorprendido\", \"asustado\", \"preocupado\", \"aburrido\", \"contento\", \"tranquilo\", \"nervioso\", \"emocionado\"]\n",
    "fem_feelings = [\"enojada\", \"cansada\", \"sorprendida\", \"asustada\", \"preocupada\", \"aburrida\", \"contenta\", \"tranquila\", \"nerviosa\", \"emocionada\"]\n",
    "\n",
    "# ------ Estaciones ------\n",
    "masc_estaciones = [\"verano\", \"otoño\", \"invierno\"]\n",
    "fem_estaciones = [\"primavera\"]\n",
    "\n",
    "# ------ Meses ------\n",
    "meses = [\"enero\", \"febrero\", \"marzo\", \"abril\", \"mayo\", \"junio\", \"julio\", \"agosto\", \"septiembre\", \"octubre\", \"noviembre\", \"diciembre\"]\n",
    "\n",
    "# ------ Días de la semana ------\n",
    "dias_semana = [\"lunes\", \"martes\", \"miércoles\", \"jueves\", \"viernes\", \"sábado\", \"domingo\"]\n",
    "\n",
    "\n",
    "words = [masc_home_places, fem_home_places, masc_outdoor_places, fem_outdoor_places, masc_work_places, fem_work_places, masc_sleep_places, fem_sleep_places, masc_tourist_places, fem_tourist_places, masc_eating_places, fem_eating_places, masc_shopping_food_places, fem_shopping_food_places, masc_health_places, fem_health_places, masc_shopping_places, fem_shopping_places, masc_transport_places, fem_transport_places, masc_family_older, fem_family_older, masc_family_younger, fem_family_younger, masc_teachers, fem_teachers, masc_friends, fem_friends, masc_medical_emergency, fem_medical_emergency, masc_first_responders, fem_first_responders, masc_first_responders_plural, fem_first_responders_plural, masc_professions, fem_professions, masc_id_objects, fem_id_objects, masc_readable_objects, fem_readable_objects, masc_readable_objects_plural, fem_readable_objects_plural, masc_personal_objects, fem_personal_objects, masc_dinero_objects, masc_toys, fem_toys, masc_objects_transport, fem_objects_transport, masc_homework_work_tools, fem_homework_work_tools, masc_work, fem_work, masc_house_pets, fem_house_pets, masc_wild_animals, fem_wild_animals, masc_insects, fem_insects, masc_events, fem_events, masc_fruits_vegetables_singular, fem_fruits_vegetables_singular, masc_fruits_vegetables_plural, fem_fruits_vegetables_plural, masc_meat, fem_meat, masc_meal, fem_meal, meals_masc, meals_fem, masc_drinks, fem_drinks, neutral_feelings, masc_feelings, fem_feelings, masc_estaciones, fem_estaciones, meses, dias_semana]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "#### *Funciones - Data Augmentation*\n",
    "\n",
    "Con el vocabulario definido, se procede a desarrollar las funciones que permitirán realizar el data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Generate Sentences ----------------- #\n",
    "# Function: generate_sentences\n",
    "# Description: Recursive function to generate all possible combinations of a sentence\n",
    "# Parameters:\n",
    "#       - original: list of words of the original sentence\n",
    "#       - current_words: list of words of the current sentence\n",
    "#       - index: index of the current word\n",
    "#       - sentences: DataFrame with the sentences generated\n",
    "# Return:\n",
    "#       - current_words: string with the current sentence\n",
    "#       - sentences: DataFrame with the sentences generated\n",
    "# ----------------------------------------------- #\n",
    "def generate_sentences(original, current_words, index, sentences):\n",
    "\n",
    "    if sentences.empty:                                 # If the DataFrame is empty add the original sentence\n",
    "        sentences = pd.concat([sentences, pd.DataFrame([\" \".join(original)], columns=[\"ORACIÓN\"])], ignore_index=True)\n",
    "\n",
    "    index_temp = index                                  # Copy the index to a temporary variable\n",
    "    found_alternative = False                           # Flag to indicate if an alternative word was found\n",
    "    \n",
    "    for word in original[index:]:                       # Iterate over the words of the original sentence\n",
    "\n",
    "        if found_alternative:                           # If an alternative word was found and changed, return the current sentence and the DataFrame with the sentences generated\n",
    "            return \" \".join(current_words), sentences   # Return the current sentence and the DataFrame with the sentences generated\n",
    "        \n",
    "        index_sublist = -1                                  \n",
    "        for i, sublist in enumerate(words):             # Iterate over the list of words\n",
    "            if word in sublist:                         # If the word is in the list of words\n",
    "                index_sublist = i                       # Save the index of the list of words\n",
    "                break\n",
    "\n",
    "        if index_sublist == -1:                         # If the word is not in the list of words, continue with the next word\n",
    "            current_words.append(word)                  # Add the word to the current sentence\n",
    "            index_temp += 1                             # Increment the index\n",
    "\n",
    "        else:                                           # If the word is in the list of words\n",
    "            found_alternative = True                    # Set the flag to True\n",
    "\n",
    "            for new_word in words[index_sublist]:       # Iterate over the words of the list of words\n",
    "\n",
    "                new_words = deepcopy(current_words)     # Copy the current words to a new variable\n",
    "                new_words.append(new_word)              # Add the new word to the current words\n",
    "\n",
    "                current_string, updated_sentences = generate_sentences(original, new_words, index_temp + 1, sentences)      # Recursive call to generate_sentences\n",
    "\n",
    "                if not updated_sentences.empty:         # If the DataFrame with the sentences generated is not empty\n",
    "                    sentences = pd.concat([sentences, updated_sentences], ignore_index=True)                                # Concatenate the DataFrame with the sentences generated\n",
    "\n",
    "                    sentences = sentences.drop_duplicates()                                                                 # Drop duplicates\n",
    "\n",
    "    if index_temp == len(original):                     # If the index is equal to the length of the original sentence\n",
    "        sentences = pd.concat([sentences, pd.DataFrame([\" \".join(current_words)], columns=[\"ORACIÓN\"])], ignore_index=True) # Concatenate the DataFrame with the current sentence\n",
    "\n",
    "        sentences = sentences.drop_duplicates()         # Drop duplicates\n",
    "\n",
    "    return \" \".join(current_words), sentences           # Return the current sentence and the DataFrame with the sentences generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Split Words ----------------- #\n",
    "# Function: split_words\n",
    "# Description: Split the words of a sentence, taking into account the punctuation\n",
    "# Parameters:\n",
    "#       - sentence: string with the sentence\n",
    "# Return:\n",
    "#       - sentence: list with the words of the sentence\n",
    "# ----------------------------------------------- #\n",
    "def split_words(sentence):\n",
    "\n",
    "    new_sentence = []                                       # List to store the words of the sentence\n",
    "    punctiation = [\",\", \".\", \";\", \"!\", \"?\", \"¿\", \"¡\"]       # List of punctuation marks\n",
    "\n",
    "    for i, char in enumerate(sentence):                     # Iterate over the characters of the sentence\n",
    "        if char in punctiation:                             # If the character is a punctuation mark\n",
    "            if i > 0 and sentence[-1] != \" \":               # If the character is not the first character and the last character is not a space\n",
    "                new_sentence.append(\" \")                    # Add a space to the list of words\n",
    "            new_sentence.append(char)                       # Add the punctuation mark to the list of words\n",
    "            new_sentence.append(\" \")                        # Add a space to the list of words\n",
    "        else:                                               # If the character is not a punctuation mark\n",
    "            new_sentence.append(char)                       # Add the character to the list of words\n",
    "\n",
    "    sentence = \"\".join(new_sentence)                        # Join the list of words to form the sentence\n",
    "\n",
    "    sentence = sentence.split()                             # Split the sentence into words\n",
    "\n",
    "    return sentence                                         # Return the list of words\n",
    "\n",
    "\n",
    "# ----------------- Correct Case ----------------- #\n",
    "# Function: correct_case\n",
    "# Description: Correct the case of the words of a sentence\n",
    "# Parameters:\n",
    "#       - sentence: string with the sentence\n",
    "# Return:\n",
    "#       - sentence: string with the sentence with the correct case\n",
    "# ----------------------------------------------- #\n",
    "def correct_case(sentence):\n",
    "\n",
    "    new_sentence = []                                       # List to store the words of the sentence\n",
    "    punctiation = [\",\", \".\", \";\", \"!\", \"?\", \"¿\", \"¡\"]       # List of punctuation marks\n",
    "\n",
    "    punctiation_bool = False                                # Flag to indicate if the character is a punctuation mark\n",
    "    remove_space = False                                    # Flag to indicate if the space should be removed\n",
    "\n",
    "    sentence = sentence.strip()                             # Remove leading and trailing whitespaces            \n",
    "\n",
    "    for i, char in enumerate(sentence):                     # Iterate over the characters of the sentence\n",
    "        if i == 0 and char.isalpha():                       # If the character is the first character and is a letter\n",
    "            new_sentence.append(char.upper())               # Add the character in uppercase to the list of words\n",
    "        elif char in punctiation:                           # If the character is a punctuation mark\n",
    "            if char == \"¿\" or char == \"¡\":                  # If the character is an inverted question or exclamation mark\n",
    "                remove_space = True                         # Set the flag to True\n",
    "            else:                                           # If the character is a punctuation mark           \n",
    "                if new_sentence[-1] == \" \":                 # If the last character is a space\n",
    "                    new_sentence.pop()                      # Remove the last character\n",
    "\n",
    "            if char != \",\":                                 # If the character is not a comma\n",
    "                punctiation_bool = True                     # Set the flag to True\n",
    "            new_sentence.append(char)                       # Add the punctuation mark to the list of words\n",
    "\n",
    "        elif punctiation_bool:                              # If the character is not a punctuation mark and the flag is True\n",
    "            if char.isalpha():                              # If the character is a letter\n",
    "                new_sentence.append(char.upper())           # Add the character in uppercase to the list of words\n",
    "                punctiation_bool = False                    # Set the flag to False\n",
    "                remove_space = False                        # Set the flag to False\n",
    "            else:                                           # If the character is not a letter\n",
    "                if not remove_space:                        # If the flag is False\n",
    "                    new_sentence.append(char)               # Add the character to the list of words\n",
    "                else:                                       # If the flag is True\n",
    "                    remove_space = False                    # Set the flag to False\n",
    "        else:                                               # If the character is not a punctuation mark           \n",
    "            new_sentence.append(char)                       # Add the character to the list of words\n",
    "\n",
    "    return \"\".join(new_sentence)                            # Return the list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "#### *Data Augmentation*\n",
    "\n",
    "Con las funciones definidas, se aplica el data augmentation al dataset original. Se generan nuevas frases a partir de las existentes, aplicando las técnicas de sustitución de palabras, cambio de tiempo verbal, y alteración de lugares y sujetos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = pd.DataFrame(columns=[\"LENSEGUA\", \"ESPAÑOL\"])                  # DataFrame to store the augmented dataset\n",
    "\n",
    "for index, row in (data.iterrows()):                                            # Iterate over the rows of the dataset\n",
    "\n",
    "    words_lensegua = split_words(row[\"LENSEGUA\"])                               # Split the words of the Lensegua sentence  \n",
    "    words_espanol = split_words(row[\"ESPAÑOL\"])                                 # Split the words of the Spanish sentence\n",
    "\n",
    "    words_found = 0                                                             # Variable to store the number of \"changeable\" words found\n",
    "\n",
    "    for word in words_espanol:                                                  # Iterate over the words of the Spanish sentence              \n",
    "        if word in [item for sublist in words for item in sublist]:             # If the word is in the list of words\n",
    "            words_found += 1                                                    # Increment the variable\n",
    "\n",
    "    if words_found == 0:                                                        # If no \"changeable\" words were found\n",
    "        new_row = pd.DataFrame({'LENSEGUA': [row[\"LENSEGUA\"]], 'ESPAÑOL': [row[\"ESPAÑOL\"]]})                # Create a new row with the original sentences\n",
    "        augmented_data = pd.concat([augmented_data, new_row], ignore_index=True)                            # Concatenate the new row to the DataFrame    \n",
    "\n",
    "    else:                                                                       # If \"changeable\" words were found\n",
    "        _, lensegua_aug = generate_sentences(words_lensegua, [], 0, pd.DataFrame(columns=[\"ORACIÓN\"]))      # Generate all possible combinations of the Lensegua sentence\n",
    "        spanish_aug = []                                                        # List to store the augmented Spanish sentences             \n",
    "\n",
    "        lensegua_aug = lensegua_aug.sort_values(by=[\"ORACIÓN\"])                 # Sort the lensegua augmented sentences\n",
    "        if len(lensegua_aug) > 20:                                              # If the length of the lensegua augmented sentences is greater than 20\n",
    "            lensegua_aug = lensegua_aug.sample(n=20)                            # Sample 20 random sentences\n",
    "\n",
    "        for index1 in lensegua_aug.index:                                       # Iterate over the indexes of the lensegua augmented sentences\n",
    "\n",
    "            aug_sentence = lensegua_aug[\"ORACIÓN\"][index1]                      # Get the augmented sentence\n",
    "            aug_sentence_split = split_words(aug_sentence)                      # Split the words of the augmented sentence\n",
    "            spanish_aug_temp = deepcopy(words_espanol)                          # Copy the words of the Spanish sentence\n",
    "\n",
    "            if len(aug_sentence_split) == len(words_lensegua):                  # If the length of the augmented sentence is equal to the length of the Lensegua sentence\n",
    "                \n",
    "                changed_id = []                                                 # List to store the indexes of the words that were changed\n",
    "\n",
    "                for word1, word2 in zip(words_lensegua, aug_sentence_split):    # Iterate over the words of the Lensegua sentence and the augmented sentence\n",
    "                    if word1 != word2:                                          # If the words are different\n",
    "                        \n",
    "                        for i, word_temp in enumerate(spanish_aug_temp):        # Iterate over the words of the Spanish sentence\n",
    "                            if word_temp == word1:                              # If the word is equal to the word of the Lensegua sentence\n",
    "\n",
    "                                if i in changed_id:                             # If the index is in the list of changed indexes\n",
    "                                    pass\n",
    "\n",
    "                                else: \n",
    "                                    spanish_aug_temp[i] = word2                 # Change the word of the Spanish sentence\n",
    "                                    changed_id.append(i)                        # Add the index to the list\n",
    "                                    break\n",
    "\n",
    "                spanish_aug.append(correct_case(\" \".join(spanish_aug_temp)))    # Add the augmented Spanish sentence to the list\n",
    "\n",
    "        spanish_aug = pd.DataFrame(spanish_aug, columns=[\"ORACIÓN\"])            # Create a DataFrame with the augmented Spanish sentences\n",
    "\n",
    "        for index1, index2 in zip(spanish_aug.index, lensegua_aug.index):       # Iterate over the indexes of the augmented Spanish sentences and the augmented Lensegua sentences\n",
    "            if len(spanish_aug) != len(lensegua_aug):                           # If the length of the augmented Spanish sentences is different from the length of the augmented Lensegua sentences\n",
    "                print(spanish_aug)  \n",
    "                print(lensegua_aug)\n",
    "\n",
    "            new_row = pd.DataFrame({'LENSEGUA': [lensegua_aug[\"ORACIÓN\"][index2]], 'ESPAÑOL': [spanish_aug[\"ORACIÓN\"][index1]]})        # Create a new row with the augmented sentences\n",
    "            augmented_data = pd.concat([augmented_data, new_row], ignore_index=True)                                                    # Concatenate the new row to the DataFrame          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LENSEGUA</th>\n",
       "      <th>ESPAÑOL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>él trabajar mucho pero ganar poco</td>\n",
       "      <td>él trabaja mucho pero gana poco.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>después artículo nosotros descansar</td>\n",
       "      <td>Después del artículo descansamos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>después documento nosotros descansar</td>\n",
       "      <td>Después del documento descansamos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>después ensayo nosotros descansar</td>\n",
       "      <td>Después del ensayo descansamos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>después examen nosotros descansar</td>\n",
       "      <td>Después del examen descansamos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>después informe nosotros descansar</td>\n",
       "      <td>Después del informe descansamos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>después proyecto nosotros descansar</td>\n",
       "      <td>Después del proyecto descansamos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>después reporte nosotros descansar</td>\n",
       "      <td>Después del reporte descansamos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>después resumen nosotros descansar</td>\n",
       "      <td>Después del resumen descansamos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ayer tu cabaña limpiar</td>\n",
       "      <td>Ayer limpiaste la cabaña.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ayer tu casa limpiar</td>\n",
       "      <td>Ayer limpiaste la casa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ayer tu choza limpiar</td>\n",
       "      <td>Ayer limpiaste la choza.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ayer tu mansión limpiar</td>\n",
       "      <td>Ayer limpiaste la mansión.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ayer tu residencia limpiar</td>\n",
       "      <td>Ayer limpiaste la residencia.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ayer tu villa limpiar</td>\n",
       "      <td>Ayer limpiaste la villa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pasado yo tarde llegar porque carro mucho</td>\n",
       "      <td>llegué tarde porque había tráfico.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tu familia vivir dónde pregunta</td>\n",
       "      <td>¿dónde vive tu familia?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mañana acuario ellos ir</td>\n",
       "      <td>Mañana ellos irán al acuario.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mañana cine ellos ir</td>\n",
       "      <td>Mañana ellos irán al cine.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>mañana estadio ellos ir</td>\n",
       "      <td>Mañana ellos irán al estadio.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     LENSEGUA  \\\n",
       "0           él trabajar mucho pero ganar poco   \n",
       "1         después artículo nosotros descansar   \n",
       "2        después documento nosotros descansar   \n",
       "3           después ensayo nosotros descansar   \n",
       "4           después examen nosotros descansar   \n",
       "5          después informe nosotros descansar   \n",
       "6         después proyecto nosotros descansar   \n",
       "7          después reporte nosotros descansar   \n",
       "8          después resumen nosotros descansar   \n",
       "9                      ayer tu cabaña limpiar   \n",
       "10                       ayer tu casa limpiar   \n",
       "11                      ayer tu choza limpiar   \n",
       "12                    ayer tu mansión limpiar   \n",
       "13                 ayer tu residencia limpiar   \n",
       "14                      ayer tu villa limpiar   \n",
       "15  pasado yo tarde llegar porque carro mucho   \n",
       "16            tu familia vivir dónde pregunta   \n",
       "17                    mañana acuario ellos ir   \n",
       "18                       mañana cine ellos ir   \n",
       "19                    mañana estadio ellos ir   \n",
       "\n",
       "                               ESPAÑOL  \n",
       "0     él trabaja mucho pero gana poco.  \n",
       "1    Después del artículo descansamos.  \n",
       "2   Después del documento descansamos.  \n",
       "3      Después del ensayo descansamos.  \n",
       "4      Después del examen descansamos.  \n",
       "5     Después del informe descansamos.  \n",
       "6    Después del proyecto descansamos.  \n",
       "7     Después del reporte descansamos.  \n",
       "8     Después del resumen descansamos.  \n",
       "9            Ayer limpiaste la cabaña.  \n",
       "10             Ayer limpiaste la casa.  \n",
       "11            Ayer limpiaste la choza.  \n",
       "12          Ayer limpiaste la mansión.  \n",
       "13       Ayer limpiaste la residencia.  \n",
       "14            Ayer limpiaste la villa.  \n",
       "15  llegué tarde porque había tráfico.  \n",
       "16             ¿dónde vive tu familia?  \n",
       "17       Mañana ellos irán al acuario.  \n",
       "18          Mañana ellos irán al cine.  \n",
       "19       Mañana ellos irán al estadio.  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_data.head(20)                                                     # Display the first 20 rows of the augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip sentences\n",
    "augmented_data[\"LENSEGUA\"] = augmented_data[\"LENSEGUA\"].apply(lambda x: x.strip())                  # Strip the Lensegua sentences\n",
    "augmented_data[\"ESPAÑOL\"] = augmented_data[\"ESPAÑOL\"].apply(lambda x: x.strip())                    # Strip the Spanish sentences\n",
    "\n",
    "validation_data[\"LENSEGUA\"] = validation_data[\"LENSEGUA\"].apply(lambda x: x.strip())                # Strip the Lensegua sentences\n",
    "validation_data[\"ESPAÑOL\"] = validation_data[\"ESPAÑOL\"].apply(lambda x: x.strip())                  # Strip the Spanish sentences\n",
    "\n",
    "# Remove '\"' characters\n",
    "augmented_data[\"LENSEGUA\"] = augmented_data[\"LENSEGUA\"].apply(lambda x: re.sub(r'\"', '', x))        # Remove double quotes\n",
    "augmented_data[\"ESPAÑOL\"] = augmented_data[\"ESPAÑOL\"].apply(lambda x: re.sub(r'\"', '', x))          # Remove double quotes\n",
    "\n",
    "validation_data[\"LENSEGUA\"] = validation_data[\"LENSEGUA\"].apply(lambda x: re.sub(r'\"', '', x))      # Remove double quotes\n",
    "validation_data[\"ESPAÑOL\"] = validation_data[\"ESPAÑOL\"].apply(lambda x: re.sub(r'\"', '', x))        # Remove double quotes\n",
    "\n",
    "# Correct case\n",
    "augmented_data[\"LENSEGUA\"] = augmented_data[\"LENSEGUA\"].apply(lambda x: correct_case(x))            # Correct the case of the Lensegua sentences\n",
    "augmented_data[\"ESPAÑOL\"] = augmented_data[\"ESPAÑOL\"].apply(lambda x: correct_case(x))              # Correct the case of the Spanish sentences\n",
    "\n",
    "validation_data[\"LENSEGUA\"] = validation_data[\"LENSEGUA\"].apply(lambda x: correct_case(x))          # Correct the case of the Lensegua sentences\n",
    "validation_data[\"ESPAÑOL\"] = validation_data[\"ESPAÑOL\"].apply(lambda x: correct_case(x))            # Correct the case of the Spanish sentences\n",
    "\n",
    "# Randomize the order of the rows\n",
    "augmented_data = augmented_data.sample(frac=1).reset_index(drop=True)                               # Randomize the order of the rows\n",
    "\n",
    "\n",
    "# Make sure the datasets are divisible by 8\n",
    "if len(augmented_data) % 8 != 0:                                                                    # If the length of the augmented dataset is not divisible by 8\n",
    "    augmented_data = augmented_data.iloc[:-(len(augmented_data) % 8)]                               # Remove the remaining rows\n",
    "\n",
    "if len(validation_data) % 8 != 0:                                                                    # If the length of the validation dataset is not divisible by 8\n",
    "    validation_data = validation_data.iloc[:-(len(validation_data) % 8)]                             # Remove the remaining rows\n",
    "\n",
    "\n",
    "augmented_data.to_csv(\"../../dataset/processed/train_data.csv\", index=False, quoting=csv.QUOTE_ALL, quotechar='\"')          # Save the augmented dataset to a CSV file\n",
    "validation_data.to_csv(\"../../dataset/processed/validation_data.csv\", index=False, quoting=csv.QUOTE_ALL, quotechar='\"')    # Save the validation dataset to a CSV file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
